{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curiosity-Driven Exploration in RL\n",
    "\n",
    "Exploration in RL is still a difficult problem, especially when applied to environments with high dimension state space such as Atari, mario, etc.\n",
    "\n",
    "Some approaches come from maintaining a [pseudo-count](https://arxiv.org/abs/1606.01868) of state observation and assigning reward bonus for less visited states.\n",
    "\n",
    "Another interesting approach, and the basis for this notebook implementation, is that of [curiosity driven exploration](https://arxiv.org/abs/1705.05363)\n",
    "\n",
    "In this work, the agent relies on a compressed (more details below) world model, and assign a reward bonus to exploring states that provide surprise as compared to it's state model. In a very crude approximation of natural curiosity (we are curious when we observe things that don't match with what we expect to happen)\n",
    "\n",
    "### Test environment\n",
    "\n",
    "Due to my limited compute, I will be testing the algorithm on the [mountain-car](https://gym.openai.com/envs/MountainCar-v0/) environment. This environment requires a precise set of actions to push the cart to the top, since it requires the agent to use the momentum of yo-yoing between the two side of the the valley to reach the top. \n",
    "\n",
    "However one caveat that should be noted is that epsilon greedy exploration can still solve the environment. The good thing is that with curiosity driven exploration, the number of epochs to solve this environment should be much faster than that of say Q-learning, and espeically for on-policy algorithm such as PPO/REINFORCE.\n",
    "\n",
    "## Curiosity Driven Exploration, ICM Module\n",
    "\n",
    "At the heart of the paper is the ICM module. I've taken a screen grab of Figure 2 below for reference. With this diagram we can begin the construct the ICM Module.\n",
    "\n",
    "![picture](img/img1.png)\n",
    "\n",
    "### components needed\n",
    "\n",
    "We can that we need to implement 3 components for ICM module. The encoder, forward model and inverse model. Below we will create these modules individually, and then join them together into one called ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    \"\"\"obseravation space encoder.\n",
    "    \n",
    "    output_dim should be smaller or equal to obs_dim\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, hidden_dim, output_dim, hidden_act=F.tanh):\n",
    "        super(FeatureEncoder,self).__init__()\n",
    "        assert(output_dim <= obs_dim), \"output_dim should be smaller or equal to input_dim\"\n",
    "        self.hidden_act = hidden_act\n",
    "        self.w1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden_act(self.w1(x))\n",
    "        x = self.w2(x) # linear output to let output to be any range of values\n",
    "        return x\n",
    "    \n",
    "class ForwardModel(nn.Module):\n",
    "    \"\"\" The forward model, which takes action and encoded observation as input and predicts\n",
    "    the next encoded state.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, embedded_dim, hidden_dim, action_dim, hidden_act=F.tanh):\n",
    "        super(ForwardModel, self).__init__()\n",
    "        self.hidden_act = hidden_act\n",
    "        self.w1 = nn.Linear(embedded_dim+action_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim, embedded_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden_act(self.w1(x))\n",
    "        x = self.w2(x)\n",
    "        return x # linear output to let output to be any range of values\n",
    "        \n",
    "class InverseModel(nn.Module):\n",
    "    \"\"\"The inverse model takes the encoded pre and post states (s_t and s_t+1) and predicts\n",
    "    and action given these. FOr now we make our model to be for discrete actions only, will\n",
    "    update it for general later.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, embedded_dim, hidden_dim, action_dim, \n",
    "                 action_type='discrete', hidden_act=F.tanh):\n",
    "        super(InverseModel, self).__init__()\n",
    "        self.hidden_act = hidden_act\n",
    "        self.action_type = action_type\n",
    "        self.w1 = nn.Linear(embedded_dim*2, hidden_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden_act(self.w1(x))\n",
    "        x = self.w2(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class ICM(object):\n",
    "    \"\"\"Overall ICM model encoporating all the above.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, embedded_dim, action_dim, \n",
    "                 hidden_dim, reward_scale=0.5, hidden_act=F.tanh):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.embedded_dim = embedded_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.reward_scale = reward_scale\n",
    "        self.encoder = FeatureEncoder(obs_dim, hidden_dim, embedded_dim, hidden_act)\n",
    "        self.i_model = InverseModel(embedded_dim, hidden_dim, act_dim)\n",
    "        self.f_model = ForwardModel(embedded_dim, hidden_dim, act_dim)\n",
    "        \n",
    "    def predict_reward(self, action, pre_state, post_state):\n",
    "        \"\"\"provides a scalar curiosity reward. This reward is the mse between our embedding\n",
    "        prediction.\n",
    "        \n",
    "        Also computes losses and return it with reward, used for loss computation\n",
    "        \n",
    "        \"\"\"\n",
    "        pre_embed = self.encoder(pre_state)\n",
    "        post_embed = self.encoder(post_state)\n",
    "        action_base = Variable(torch.zeros(self.action_dim))\n",
    "        # get one hot encoding action\n",
    "        action_base[action] = 1.0\n",
    "        # concatenate pre_embedding and action to feed to our forward model\n",
    "        f_model_feed = torch.cat([pre_embed.view(1,-1), action_base.view(1,-1)],dim=1)\n",
    "        f_model_pre_embed = self.f_model(f_model_feed)\n",
    "        \n",
    "        # compute inverse model action, need concatenate pre_embedding and post embedding\n",
    "        i_model_feed = torch.cat([pre_embed.view(1,-1), post_embed.view(1,-1)], dim=1)\n",
    "        \n",
    "        # compute action loss, for discrete action space this is the negative_log_loss/cross entorpy loss\n",
    "        i_model_action_pred = self.i_model(i_model_feed) # note this is unnormalized prediction\n",
    "        action_pred = F.softmax(i_model_action_pred, dim=1) # softmax action distribution\n",
    "        action_pred_log = F.log_softmax(i_model_action_pred, dim=1) # stable log_softmax\n",
    "        action_loss = F.nll_loss(action_pred, action) # Negative log-likelihood loss\n",
    "        \n",
    "        # compute the embedding prediction loss, this is the MSE loss between our forward model\n",
    "        # prediction of embedding versus actual embedding obtained\n",
    "        # note in this case we're holding the post embedding as a constant, we can't have both\n",
    "        # prediction and target in our loss to be tunable.\n",
    "        embedding_loss = F.smooth_l1_loss(f_model_pre_embed, Variable(post_embed.data))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return embedding_loss, action_loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-11 14:47:04,918] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " 1.00000e-03 *\n",
       "   7.9127\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       " -0.3067\n",
       " [torch.FloatTensor of size 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "embedded_dim = 1\n",
    "icm = ICM(obs_dim, embedded_dim, act_dim, 10)\n",
    "sample_obs = Variable(torch.FloatTensor(env.observation_space.sample()).view(1,-1))\n",
    "sample_action = Variable(torch.LongTensor([env.action_space.sample()]))\n",
    "\n",
    "icm.predict_reward(sample_action, sample_obs, sample_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F.smooth_l1_loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch0.3]",
   "language": "python",
   "name": "conda-env-Pytorch0.3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
